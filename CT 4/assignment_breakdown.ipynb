{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC 580: Critical Thinking 4 - Toxicology Testing\n",
    "TODO - Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from deepchem import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Seeds are set via assignment parameters\n",
    "np.random.seed(456)\n",
    "tf.random.set_seed(456)\n",
    "\n",
    "# Disables eager execution so TF v1 code can be run\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data\n",
    "Using the [Tox21 Dataset](https://tox21.gov/resources/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,(train, valid, test),_ = dc.molnet.load_tox21()\n",
    "\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y[:,0]\n",
    "valid_y = valid_y[:,0]\n",
    "test_y = test_y[:,0]\n",
    "\n",
    "train_w = train_w[:,0]\n",
    "valid_w = valid_w[:,0]\n",
    "test_w = test_w[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/CSUG/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "d = 1024 # Dimensionality of the feature vector\n",
    "n_hidden = 50\n",
    "learning_rate = .001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "dropout_prob = 0.5\n",
    "\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.compat.v1.placeholder(tf.float32, (None, d))\n",
    "    y = tf.compat.v1.placeholder(tf.float32, (None,))\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32) # Dropout placeholder\n",
    "\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W = tf.compat.v1.Variable(tf.compat.v1.random_normal((d, n_hidden)))\n",
    "    b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden,)))\n",
    "    x_hidden = tf.compat.v1.nn.relu(tf.compat.v1.matmul(x,W) + b)\n",
    "    x_hidden = tf.compat.v1.nn.dropout(x_hidden, keep_prob) # Applying dropout\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden, 1)))\n",
    "    b = tf.compat.v1.Variable(tf.compat.v1.random_normal((1,)))\n",
    "    y_logit = tf.compat.v1.matmul(x_hidden,W) + b\n",
    "\n",
    "    # The sigmoid gives the class probability of 1\n",
    "    y_one_prob = tf.compat.v1.sigmoid(y_logit)\n",
    "\n",
    "    # Rounding P(y=1) will give the correct prediction\n",
    "    y_pred = tf.compat.v1.round(y_one_prob)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # Compute the cross-entropy term for each datapoint\n",
    "    y_expand = tf.compat.v1.expand_dims(y, 1)\n",
    "    entropy = tf.compat.v1.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "\n",
    "    # Sum all contributions\n",
    "    l = tf.compat.v1.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.compat.v1.summary.scalar(\"loss\", l)\n",
    "    merged = tf.compat.v1.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 0, loss 1291.876831\n",
      "epoch 0, step 1, loss 970.218201\n",
      "epoch 0, step 2, loss 1134.187500\n",
      "epoch 0, step 3, loss 1120.142944\n",
      "epoch 0, step 4, loss 1176.735840\n",
      "epoch 0, step 5, loss 1192.459595\n",
      "epoch 0, step 6, loss 1029.912598\n",
      "epoch 0, step 7, loss 1315.102783\n",
      "epoch 0, step 8, loss 1131.226807\n",
      "epoch 0, step 9, loss 1034.675049\n",
      "epoch 0, step 10, loss 1000.422241\n",
      "epoch 0, step 11, loss 799.628784\n",
      "epoch 0, step 12, loss 1245.834106\n",
      "epoch 0, step 13, loss 1064.413086\n",
      "epoch 0, step 14, loss 1029.998779\n",
      "epoch 0, step 15, loss 984.309814\n",
      "epoch 0, step 16, loss 1367.160156\n",
      "epoch 0, step 17, loss 1185.448120\n",
      "epoch 0, step 18, loss 1671.988037\n",
      "epoch 0, step 19, loss 1672.233154\n",
      "epoch 0, step 20, loss 1529.798340\n",
      "epoch 0, step 21, loss 1524.408203\n",
      "epoch 0, step 22, loss 1682.842896\n",
      "epoch 0, step 23, loss 1499.609863\n",
      "epoch 0, step 24, loss 1253.440918\n",
      "epoch 0, step 25, loss 1446.849365\n",
      "epoch 0, step 26, loss 1204.458740\n",
      "epoch 0, step 27, loss 1643.072144\n",
      "epoch 0, step 28, loss 1346.530518\n",
      "epoch 0, step 29, loss 948.648315\n",
      "epoch 0, step 30, loss 1164.949707\n",
      "epoch 0, step 31, loss 1058.148193\n",
      "epoch 0, step 32, loss 1506.167114\n",
      "epoch 0, step 33, loss 1173.751099\n",
      "epoch 0, step 34, loss 1444.216553\n",
      "epoch 0, step 35, loss 1187.104492\n",
      "epoch 0, step 36, loss 1103.983521\n",
      "epoch 0, step 37, loss 1415.814209\n",
      "epoch 0, step 38, loss 904.427063\n",
      "epoch 0, step 39, loss 1515.715088\n",
      "epoch 0, step 40, loss 834.668213\n",
      "epoch 0, step 41, loss 1111.128540\n",
      "epoch 0, step 42, loss 720.854126\n",
      "epoch 0, step 43, loss 833.185913\n",
      "epoch 0, step 44, loss 900.811035\n",
      "epoch 0, step 45, loss 821.443359\n",
      "epoch 0, step 46, loss 856.173828\n",
      "epoch 0, step 47, loss 1032.976318\n",
      "epoch 0, step 48, loss 945.559448\n",
      "epoch 0, step 49, loss 970.100952\n",
      "epoch 0, step 50, loss 1031.357666\n",
      "epoch 0, step 51, loss 770.184998\n",
      "epoch 0, step 52, loss 795.846985\n",
      "epoch 0, step 53, loss 751.528809\n",
      "epoch 0, step 54, loss 724.608521\n",
      "epoch 0, step 55, loss 753.276245\n",
      "epoch 0, step 56, loss 856.052734\n",
      "epoch 0, step 57, loss 646.205078\n",
      "epoch 0, step 58, loss 927.455750\n",
      "epoch 0, step 59, loss 609.356079\n",
      "epoch 0, step 60, loss 426.650421\n",
      "epoch 0, step 61, loss 618.577026\n",
      "epoch 0, step 62, loss 341.506897\n",
      "Train Weighted Classification Accuracy: 0.459120\n",
      "Valid Weighted Classification Accuracy: 0.517158\n",
      "epoch 1, step 63, loss 493.970032\n",
      "epoch 1, step 64, loss 472.972534\n",
      "epoch 1, step 65, loss 570.065430\n",
      "epoch 1, step 66, loss 558.030334\n",
      "epoch 1, step 67, loss 530.033997\n",
      "epoch 1, step 68, loss 409.454773\n",
      "epoch 1, step 69, loss 695.714172\n",
      "epoch 1, step 70, loss 538.635254\n",
      "epoch 1, step 71, loss 524.434204\n",
      "epoch 1, step 72, loss 452.163513\n",
      "epoch 1, step 73, loss 425.470245\n",
      "epoch 1, step 74, loss 482.195587\n",
      "epoch 1, step 75, loss 638.184937\n",
      "epoch 1, step 76, loss 310.914124\n",
      "epoch 1, step 77, loss 429.703125\n",
      "epoch 1, step 78, loss 357.739563\n",
      "epoch 1, step 79, loss 549.027832\n",
      "epoch 1, step 80, loss 432.288086\n",
      "epoch 1, step 81, loss 624.168579\n",
      "epoch 1, step 82, loss 628.164795\n",
      "epoch 1, step 83, loss 627.049072\n",
      "epoch 1, step 84, loss 576.289307\n",
      "epoch 1, step 85, loss 632.542297\n",
      "epoch 1, step 86, loss 736.983276\n",
      "epoch 1, step 87, loss 479.337280\n",
      "epoch 1, step 88, loss 585.104126\n",
      "epoch 1, step 89, loss 498.423645\n",
      "epoch 1, step 90, loss 497.087830\n",
      "epoch 1, step 91, loss 496.768616\n",
      "epoch 1, step 92, loss 341.633179\n",
      "epoch 1, step 93, loss 542.137939\n",
      "epoch 1, step 94, loss 380.458191\n",
      "epoch 1, step 95, loss 580.719482\n",
      "epoch 1, step 96, loss 399.636230\n",
      "epoch 1, step 97, loss 438.482422\n",
      "epoch 1, step 98, loss 475.644226\n",
      "epoch 1, step 99, loss 1096.224854\n",
      "epoch 1, step 100, loss 1560.644653\n",
      "epoch 1, step 101, loss 408.500671\n",
      "epoch 1, step 102, loss 418.787903\n",
      "epoch 1, step 103, loss 328.536560\n",
      "epoch 1, step 104, loss 757.892822\n",
      "epoch 1, step 105, loss 348.661133\n",
      "epoch 1, step 106, loss 345.189301\n",
      "epoch 1, step 107, loss 412.826691\n",
      "epoch 1, step 108, loss 591.940674\n",
      "epoch 1, step 109, loss 179.446777\n",
      "epoch 1, step 110, loss 629.426147\n",
      "epoch 1, step 111, loss 431.527252\n",
      "epoch 1, step 112, loss 376.896729\n",
      "epoch 1, step 113, loss 705.267578\n",
      "epoch 1, step 114, loss 577.339478\n",
      "epoch 1, step 115, loss 358.708435\n",
      "epoch 1, step 116, loss 543.787476\n",
      "epoch 1, step 117, loss 346.442200\n",
      "epoch 1, step 118, loss 390.945557\n",
      "epoch 1, step 119, loss 630.403198\n",
      "epoch 1, step 120, loss 290.896179\n",
      "epoch 1, step 121, loss 503.866241\n",
      "epoch 1, step 122, loss 461.820801\n",
      "epoch 1, step 123, loss 292.356506\n",
      "epoch 1, step 124, loss 380.431366\n",
      "epoch 1, step 125, loss 124.550888\n",
      "Train Weighted Classification Accuracy: 0.482687\n",
      "Valid Weighted Classification Accuracy: 0.518719\n",
      "epoch 2, step 126, loss 258.450897\n",
      "epoch 2, step 127, loss 320.512329\n",
      "epoch 2, step 128, loss 197.590485\n",
      "epoch 2, step 129, loss 149.809738\n",
      "epoch 2, step 130, loss 298.989227\n",
      "epoch 2, step 131, loss 111.073280\n",
      "epoch 2, step 132, loss 150.358490\n",
      "epoch 2, step 133, loss 346.896698\n",
      "epoch 2, step 134, loss 426.084106\n",
      "epoch 2, step 135, loss 131.266144\n",
      "epoch 2, step 136, loss 240.187225\n",
      "epoch 2, step 137, loss 366.304077\n",
      "epoch 2, step 138, loss 179.261932\n",
      "epoch 2, step 139, loss 165.032196\n",
      "epoch 2, step 140, loss 219.565704\n",
      "epoch 2, step 141, loss 206.650604\n",
      "epoch 2, step 142, loss 221.941040\n",
      "epoch 2, step 143, loss 231.490707\n",
      "epoch 2, step 144, loss 367.916779\n",
      "epoch 2, step 145, loss 247.606598\n",
      "epoch 2, step 146, loss 280.330139\n",
      "epoch 2, step 147, loss 202.216064\n",
      "epoch 2, step 148, loss 287.989838\n",
      "epoch 2, step 149, loss 246.225800\n",
      "epoch 2, step 150, loss 298.081207\n",
      "epoch 2, step 151, loss 349.942261\n",
      "epoch 2, step 152, loss 294.001160\n",
      "epoch 2, step 153, loss 314.880035\n",
      "epoch 2, step 154, loss 330.424652\n",
      "epoch 2, step 155, loss 132.735596\n",
      "epoch 2, step 156, loss 260.279816\n",
      "epoch 2, step 157, loss 319.277222\n",
      "epoch 2, step 158, loss 272.562073\n",
      "epoch 2, step 159, loss 215.447510\n",
      "epoch 2, step 160, loss 199.541885\n",
      "epoch 2, step 161, loss 363.948669\n",
      "epoch 2, step 162, loss 1125.244629\n",
      "epoch 2, step 163, loss 1031.370728\n",
      "epoch 2, step 164, loss 244.700653\n",
      "epoch 2, step 165, loss 281.169220\n",
      "epoch 2, step 166, loss 232.184509\n",
      "epoch 2, step 167, loss 327.249115\n",
      "epoch 2, step 168, loss 304.790100\n",
      "epoch 2, step 169, loss 213.378754\n",
      "epoch 2, step 170, loss 276.396942\n",
      "epoch 2, step 171, loss 478.145905\n",
      "epoch 2, step 172, loss 232.041138\n",
      "epoch 2, step 173, loss 687.492676\n",
      "epoch 2, step 174, loss 323.406799\n",
      "epoch 2, step 175, loss 427.231018\n",
      "epoch 2, step 176, loss 452.605164\n",
      "epoch 2, step 177, loss 460.413025\n",
      "epoch 2, step 178, loss 444.815399\n",
      "epoch 2, step 179, loss 487.740234\n",
      "epoch 2, step 180, loss 188.491623\n",
      "epoch 2, step 181, loss 193.492279\n",
      "epoch 2, step 182, loss 297.833496\n",
      "epoch 2, step 183, loss 266.060852\n",
      "epoch 2, step 184, loss 352.091431\n",
      "epoch 2, step 185, loss 552.189331\n",
      "epoch 2, step 186, loss 281.889404\n",
      "epoch 2, step 187, loss 291.318970\n",
      "epoch 2, step 188, loss 227.384094\n",
      "Train Weighted Classification Accuracy: 0.497282\n",
      "Valid Weighted Classification Accuracy: 0.517842\n",
      "epoch 3, step 189, loss 91.958740\n",
      "epoch 3, step 190, loss 230.097214\n",
      "epoch 3, step 191, loss 161.347748\n",
      "epoch 3, step 192, loss 200.647125\n",
      "epoch 3, step 193, loss 89.123993\n",
      "epoch 3, step 194, loss 37.589943\n",
      "epoch 3, step 195, loss 166.214645\n",
      "epoch 3, step 196, loss 110.991791\n",
      "epoch 3, step 197, loss 157.126038\n",
      "epoch 3, step 198, loss 120.787415\n",
      "epoch 3, step 199, loss 164.027832\n",
      "epoch 3, step 200, loss 263.251953\n",
      "epoch 3, step 201, loss 128.116486\n",
      "epoch 3, step 202, loss 92.004639\n",
      "epoch 3, step 203, loss 139.610962\n",
      "epoch 3, step 204, loss 184.966644\n",
      "epoch 3, step 205, loss 162.344559\n",
      "epoch 3, step 206, loss 150.713959\n",
      "epoch 3, step 207, loss 317.882599\n",
      "epoch 3, step 208, loss 105.100792\n",
      "epoch 3, step 209, loss 134.572540\n",
      "epoch 3, step 210, loss 100.897484\n",
      "epoch 3, step 211, loss 342.506836\n",
      "epoch 3, step 212, loss 257.038330\n",
      "epoch 3, step 213, loss 136.180328\n",
      "epoch 3, step 214, loss 282.222595\n",
      "epoch 3, step 215, loss 132.812714\n",
      "epoch 3, step 216, loss 141.959946\n",
      "epoch 3, step 217, loss 216.581390\n",
      "epoch 3, step 218, loss 95.651093\n",
      "epoch 3, step 219, loss 200.481201\n",
      "epoch 3, step 220, loss 88.060974\n",
      "epoch 3, step 221, loss 227.685211\n",
      "epoch 3, step 222, loss 100.390320\n",
      "epoch 3, step 223, loss 205.033997\n",
      "epoch 3, step 224, loss 183.337418\n",
      "epoch 3, step 225, loss 940.184448\n",
      "epoch 3, step 226, loss 938.863403\n",
      "epoch 3, step 227, loss 333.026093\n",
      "epoch 3, step 228, loss 232.912476\n",
      "epoch 3, step 229, loss 204.308289\n",
      "epoch 3, step 230, loss 391.885254\n",
      "epoch 3, step 231, loss 299.826172\n",
      "epoch 3, step 232, loss 214.720612\n",
      "epoch 3, step 233, loss 325.798004\n",
      "epoch 3, step 234, loss 535.352173\n",
      "epoch 3, step 235, loss 100.876175\n",
      "epoch 3, step 236, loss 384.158325\n",
      "epoch 3, step 237, loss 363.788574\n",
      "epoch 3, step 238, loss 290.944916\n",
      "epoch 3, step 239, loss 389.766876\n",
      "epoch 3, step 240, loss 693.312012\n",
      "epoch 3, step 241, loss 416.405029\n",
      "epoch 3, step 242, loss 508.141724\n",
      "epoch 3, step 243, loss 450.864777\n",
      "epoch 3, step 244, loss 334.541870\n",
      "epoch 3, step 245, loss 421.325134\n",
      "epoch 3, step 246, loss 167.477341\n",
      "epoch 3, step 247, loss 370.220276\n",
      "epoch 3, step 248, loss 413.594666\n",
      "epoch 3, step 249, loss 267.771118\n",
      "epoch 3, step 250, loss 315.515625\n",
      "epoch 3, step 251, loss 169.729889\n",
      "Train Weighted Classification Accuracy: 0.507256\n",
      "Valid Weighted Classification Accuracy: 0.525804\n",
      "epoch 4, step 252, loss 65.653084\n",
      "epoch 4, step 253, loss 144.166718\n",
      "epoch 4, step 254, loss 93.552185\n",
      "epoch 4, step 255, loss 173.686615\n",
      "epoch 4, step 256, loss 91.453514\n",
      "epoch 4, step 257, loss 69.283081\n",
      "epoch 4, step 258, loss 130.541718\n",
      "epoch 4, step 259, loss 81.776031\n",
      "epoch 4, step 260, loss 159.786072\n",
      "epoch 4, step 261, loss 122.537560\n",
      "epoch 4, step 262, loss 114.278938\n",
      "epoch 4, step 263, loss 180.542236\n",
      "epoch 4, step 264, loss 69.518921\n",
      "epoch 4, step 265, loss 88.720123\n",
      "epoch 4, step 266, loss 192.762833\n",
      "epoch 4, step 267, loss 175.468292\n",
      "epoch 4, step 268, loss 109.198845\n",
      "epoch 4, step 269, loss 77.305305\n",
      "epoch 4, step 270, loss 320.715759\n",
      "epoch 4, step 271, loss 59.427425\n",
      "epoch 4, step 272, loss 196.922073\n",
      "epoch 4, step 273, loss 95.109879\n",
      "epoch 4, step 274, loss 201.320496\n",
      "epoch 4, step 275, loss 162.505005\n",
      "epoch 4, step 276, loss 124.628929\n",
      "epoch 4, step 277, loss 93.953270\n",
      "epoch 4, step 278, loss 54.618477\n",
      "epoch 4, step 279, loss 101.757500\n",
      "epoch 4, step 280, loss 292.895691\n",
      "epoch 4, step 281, loss 124.912216\n",
      "epoch 4, step 282, loss 114.652168\n",
      "epoch 4, step 283, loss 95.678856\n",
      "epoch 4, step 284, loss 169.317245\n",
      "epoch 4, step 285, loss 99.160210\n",
      "epoch 4, step 286, loss 128.174957\n",
      "epoch 4, step 287, loss 173.612091\n",
      "epoch 4, step 288, loss 1033.062134\n",
      "epoch 4, step 289, loss 1124.633545\n",
      "epoch 4, step 290, loss 233.171692\n",
      "epoch 4, step 291, loss 176.942413\n",
      "epoch 4, step 292, loss 193.003922\n",
      "epoch 4, step 293, loss 347.345306\n",
      "epoch 4, step 294, loss 309.973480\n",
      "epoch 4, step 295, loss 89.751419\n",
      "epoch 4, step 296, loss 269.093811\n",
      "epoch 4, step 297, loss 366.210571\n",
      "epoch 4, step 298, loss 109.314064\n",
      "epoch 4, step 299, loss 512.097229\n",
      "epoch 4, step 300, loss 325.142303\n",
      "epoch 4, step 301, loss 160.507843\n",
      "epoch 4, step 302, loss 303.517395\n",
      "epoch 4, step 303, loss 484.990356\n",
      "epoch 4, step 304, loss 396.034485\n",
      "epoch 4, step 305, loss 509.944092\n",
      "epoch 4, step 306, loss 269.824402\n",
      "epoch 4, step 307, loss 135.925095\n",
      "epoch 4, step 308, loss 549.031006\n",
      "epoch 4, step 309, loss 204.439850\n",
      "epoch 4, step 310, loss 303.882629\n",
      "epoch 4, step 311, loss 366.844055\n",
      "epoch 4, step 312, loss 187.029007\n",
      "epoch 4, step 313, loss 304.347809\n",
      "epoch 4, step 314, loss 157.281647\n",
      "Train Weighted Classification Accuracy: 0.522149\n",
      "Valid Weighted Classification Accuracy: 0.527975\n",
      "epoch 5, step 315, loss 89.843231\n",
      "epoch 5, step 316, loss 110.555115\n",
      "epoch 5, step 317, loss 104.120300\n",
      "epoch 5, step 318, loss 138.244278\n",
      "epoch 5, step 319, loss 87.673904\n",
      "epoch 5, step 320, loss 85.600677\n",
      "epoch 5, step 321, loss 76.590187\n",
      "epoch 5, step 322, loss 67.427849\n",
      "epoch 5, step 323, loss 55.534908\n",
      "epoch 5, step 324, loss 80.928047\n",
      "epoch 5, step 325, loss 115.440628\n",
      "epoch 5, step 326, loss 188.896011\n",
      "epoch 5, step 327, loss 52.794632\n",
      "epoch 5, step 328, loss 54.485176\n",
      "epoch 5, step 329, loss 184.658859\n",
      "epoch 5, step 330, loss 166.998962\n",
      "epoch 5, step 331, loss 78.124207\n",
      "epoch 5, step 332, loss 94.216377\n",
      "epoch 5, step 333, loss 155.398880\n",
      "epoch 5, step 334, loss 58.533936\n",
      "epoch 5, step 335, loss 138.587402\n",
      "epoch 5, step 336, loss 38.285011\n",
      "epoch 5, step 337, loss 119.967171\n",
      "epoch 5, step 338, loss 118.052444\n",
      "epoch 5, step 339, loss 159.225220\n",
      "epoch 5, step 340, loss 145.808975\n",
      "epoch 5, step 341, loss 35.851364\n",
      "epoch 5, step 342, loss 150.319244\n",
      "epoch 5, step 343, loss 201.761444\n",
      "epoch 5, step 344, loss 101.784691\n",
      "epoch 5, step 345, loss 120.643578\n",
      "epoch 5, step 346, loss 79.421707\n",
      "epoch 5, step 347, loss 127.182068\n",
      "epoch 5, step 348, loss 142.595856\n",
      "epoch 5, step 349, loss 90.926315\n",
      "epoch 5, step 350, loss 148.879883\n",
      "epoch 5, step 351, loss 965.558594\n",
      "epoch 5, step 352, loss 986.587524\n",
      "epoch 5, step 353, loss 389.346252\n",
      "epoch 5, step 354, loss 193.674896\n",
      "epoch 5, step 355, loss 239.695236\n",
      "epoch 5, step 356, loss 441.639740\n",
      "epoch 5, step 357, loss 288.703949\n",
      "epoch 5, step 358, loss 142.588165\n",
      "epoch 5, step 359, loss 348.545807\n",
      "epoch 5, step 360, loss 350.268768\n",
      "epoch 5, step 361, loss 179.547821\n",
      "epoch 5, step 362, loss 430.288757\n",
      "epoch 5, step 363, loss 187.135956\n",
      "epoch 5, step 364, loss 154.194672\n",
      "epoch 5, step 365, loss 343.740356\n",
      "epoch 5, step 366, loss 362.765564\n",
      "epoch 5, step 367, loss 246.952576\n",
      "epoch 5, step 368, loss 234.723480\n",
      "epoch 5, step 369, loss 250.089935\n",
      "epoch 5, step 370, loss 190.996445\n",
      "epoch 5, step 371, loss 440.781738\n",
      "epoch 5, step 372, loss 131.641327\n",
      "epoch 5, step 373, loss 238.544281\n",
      "epoch 5, step 374, loss 439.632904\n",
      "epoch 5, step 375, loss 251.215134\n",
      "epoch 5, step 376, loss 229.526978\n",
      "epoch 5, step 377, loss 91.913315\n",
      "Train Weighted Classification Accuracy: 0.527747\n",
      "Valid Weighted Classification Accuracy: 0.530147\n",
      "epoch 6, step 378, loss 42.969624\n",
      "epoch 6, step 379, loss 186.533661\n",
      "epoch 6, step 380, loss 62.063622\n",
      "epoch 6, step 381, loss 181.085693\n",
      "epoch 6, step 382, loss 84.908920\n",
      "epoch 6, step 383, loss 37.783211\n",
      "epoch 6, step 384, loss 45.627220\n",
      "epoch 6, step 385, loss 51.954281\n",
      "epoch 6, step 386, loss 30.904125\n",
      "epoch 6, step 387, loss 47.830780\n",
      "epoch 6, step 388, loss 60.611107\n",
      "epoch 6, step 389, loss 248.043579\n",
      "epoch 6, step 390, loss 52.582619\n",
      "epoch 6, step 391, loss 29.671919\n",
      "epoch 6, step 392, loss 109.760719\n",
      "epoch 6, step 393, loss 207.537964\n",
      "epoch 6, step 394, loss 56.961716\n",
      "epoch 6, step 395, loss 79.043823\n",
      "epoch 6, step 396, loss 331.241302\n",
      "epoch 6, step 397, loss 40.683491\n",
      "epoch 6, step 398, loss 172.029541\n",
      "epoch 6, step 399, loss 24.015989\n",
      "epoch 6, step 400, loss 49.313435\n",
      "epoch 6, step 401, loss 89.508171\n",
      "epoch 6, step 402, loss 120.102570\n",
      "epoch 6, step 403, loss 149.538666\n",
      "epoch 6, step 404, loss 43.659443\n",
      "epoch 6, step 405, loss 33.981857\n",
      "epoch 6, step 406, loss 148.012512\n",
      "epoch 6, step 407, loss 91.650230\n",
      "epoch 6, step 408, loss 29.820110\n",
      "epoch 6, step 409, loss 59.764828\n",
      "epoch 6, step 410, loss 170.889877\n",
      "epoch 6, step 411, loss 115.957169\n",
      "epoch 6, step 412, loss 98.908134\n",
      "epoch 6, step 413, loss 159.624451\n",
      "epoch 6, step 414, loss 943.672729\n",
      "epoch 6, step 415, loss 710.670898\n",
      "epoch 6, step 416, loss 231.577515\n",
      "epoch 6, step 417, loss 220.922379\n",
      "epoch 6, step 418, loss 133.367188\n",
      "epoch 6, step 419, loss 411.156677\n",
      "epoch 6, step 420, loss 217.821075\n",
      "epoch 6, step 421, loss 197.998932\n",
      "epoch 6, step 422, loss 261.783569\n",
      "epoch 6, step 423, loss 306.370911\n",
      "epoch 6, step 424, loss 79.889053\n",
      "epoch 6, step 425, loss 488.956238\n",
      "epoch 6, step 426, loss 284.215210\n",
      "epoch 6, step 427, loss 186.735138\n",
      "epoch 6, step 428, loss 318.429260\n",
      "epoch 6, step 429, loss 326.257935\n",
      "epoch 6, step 430, loss 356.891541\n",
      "epoch 6, step 431, loss 289.945435\n",
      "epoch 6, step 432, loss 266.635101\n",
      "epoch 6, step 433, loss 128.468445\n",
      "epoch 6, step 434, loss 373.420654\n",
      "epoch 6, step 435, loss 182.716248\n",
      "epoch 6, step 436, loss 268.987152\n",
      "epoch 6, step 437, loss 330.984650\n",
      "epoch 6, step 438, loss 303.694580\n",
      "epoch 6, step 439, loss 125.498863\n",
      "epoch 6, step 440, loss 135.106430\n",
      "Train Weighted Classification Accuracy: 0.546482\n",
      "Valid Weighted Classification Accuracy: 0.530871\n",
      "epoch 7, step 441, loss 66.388725\n",
      "epoch 7, step 442, loss 164.599701\n",
      "epoch 7, step 443, loss 96.300819\n",
      "epoch 7, step 444, loss 179.230484\n",
      "epoch 7, step 445, loss 121.165993\n",
      "epoch 7, step 446, loss 49.414875\n",
      "epoch 7, step 447, loss 58.367023\n",
      "epoch 7, step 448, loss 85.521774\n",
      "epoch 7, step 449, loss 77.550522\n",
      "epoch 7, step 450, loss 67.725891\n",
      "epoch 7, step 451, loss 86.984528\n",
      "epoch 7, step 452, loss 208.016327\n",
      "epoch 7, step 453, loss 16.758490\n",
      "epoch 7, step 454, loss 64.374756\n",
      "epoch 7, step 455, loss 59.509495\n",
      "epoch 7, step 456, loss 176.090164\n",
      "epoch 7, step 457, loss 45.007729\n",
      "epoch 7, step 458, loss 74.413475\n",
      "epoch 7, step 459, loss 221.611664\n",
      "epoch 7, step 460, loss 48.148178\n",
      "epoch 7, step 461, loss 76.830780\n",
      "epoch 7, step 462, loss 38.411446\n",
      "epoch 7, step 463, loss 125.976669\n",
      "epoch 7, step 464, loss 119.311417\n",
      "epoch 7, step 465, loss 92.060982\n",
      "epoch 7, step 466, loss 119.458260\n",
      "epoch 7, step 467, loss 29.629051\n",
      "epoch 7, step 468, loss 70.216309\n",
      "epoch 7, step 469, loss 244.034790\n",
      "epoch 7, step 470, loss 88.253365\n",
      "epoch 7, step 471, loss 70.548790\n",
      "epoch 7, step 472, loss 39.602947\n",
      "epoch 7, step 473, loss 90.558990\n",
      "epoch 7, step 474, loss 67.863373\n",
      "epoch 7, step 475, loss 131.902542\n",
      "epoch 7, step 476, loss 65.876923\n",
      "epoch 7, step 477, loss 716.463867\n",
      "epoch 7, step 478, loss 657.237671\n",
      "epoch 7, step 479, loss 213.800934\n",
      "epoch 7, step 480, loss 119.584114\n",
      "epoch 7, step 481, loss 127.796844\n",
      "epoch 7, step 482, loss 471.375702\n",
      "epoch 7, step 483, loss 370.888489\n",
      "epoch 7, step 484, loss 195.683243\n",
      "epoch 7, step 485, loss 211.495316\n",
      "epoch 7, step 486, loss 254.525574\n",
      "epoch 7, step 487, loss 113.934692\n",
      "epoch 7, step 488, loss 306.731750\n",
      "epoch 7, step 489, loss 162.509689\n",
      "epoch 7, step 490, loss 122.261398\n",
      "epoch 7, step 491, loss 540.714844\n",
      "epoch 7, step 492, loss 384.966187\n",
      "epoch 7, step 493, loss 324.768616\n",
      "epoch 7, step 494, loss 227.820557\n",
      "epoch 7, step 495, loss 217.851151\n",
      "epoch 7, step 496, loss 129.246735\n",
      "epoch 7, step 497, loss 419.211670\n",
      "epoch 7, step 498, loss 240.621429\n",
      "epoch 7, step 499, loss 212.372284\n",
      "epoch 7, step 500, loss 429.259460\n",
      "epoch 7, step 501, loss 269.582825\n",
      "epoch 7, step 502, loss 212.824112\n",
      "epoch 7, step 503, loss 91.556091\n",
      "Train Weighted Classification Accuracy: 0.564590\n",
      "Valid Weighted Classification Accuracy: 0.563025\n",
      "epoch 8, step 504, loss 8.422812\n",
      "epoch 8, step 505, loss 120.133728\n",
      "epoch 8, step 506, loss 109.710999\n",
      "epoch 8, step 507, loss 81.036713\n",
      "epoch 8, step 508, loss 77.212875\n",
      "epoch 8, step 509, loss 44.788784\n",
      "epoch 8, step 510, loss 44.541832\n",
      "epoch 8, step 511, loss 47.036518\n",
      "epoch 8, step 512, loss 72.474861\n",
      "epoch 8, step 513, loss 44.991531\n",
      "epoch 8, step 514, loss 34.909313\n",
      "epoch 8, step 515, loss 153.305603\n",
      "epoch 8, step 516, loss 43.450127\n",
      "epoch 8, step 517, loss 51.152569\n",
      "epoch 8, step 518, loss 75.153343\n",
      "epoch 8, step 519, loss 155.566284\n",
      "epoch 8, step 520, loss 64.460251\n",
      "epoch 8, step 521, loss 51.564751\n",
      "epoch 8, step 522, loss 246.554916\n",
      "epoch 8, step 523, loss 60.733604\n",
      "epoch 8, step 524, loss 101.174690\n",
      "epoch 8, step 525, loss 48.049805\n",
      "epoch 8, step 526, loss 92.434525\n",
      "epoch 8, step 527, loss 77.310150\n",
      "epoch 8, step 528, loss 90.982681\n",
      "epoch 8, step 529, loss 76.028885\n",
      "epoch 8, step 530, loss 14.363950\n",
      "epoch 8, step 531, loss 82.288521\n",
      "epoch 8, step 532, loss 245.092926\n",
      "epoch 8, step 533, loss 110.425354\n",
      "epoch 8, step 534, loss 56.204845\n",
      "epoch 8, step 535, loss 132.869034\n",
      "epoch 8, step 536, loss 65.790977\n",
      "epoch 8, step 537, loss 99.538849\n",
      "epoch 8, step 538, loss 26.156878\n",
      "epoch 8, step 539, loss 145.127777\n",
      "epoch 8, step 540, loss 621.229309\n",
      "epoch 8, step 541, loss 583.805908\n",
      "epoch 8, step 542, loss 279.275269\n",
      "epoch 8, step 543, loss 120.475754\n",
      "epoch 8, step 544, loss 79.006989\n",
      "epoch 8, step 545, loss 421.769379\n",
      "epoch 8, step 546, loss 272.328857\n",
      "epoch 8, step 547, loss 182.096176\n",
      "epoch 8, step 548, loss 263.169830\n",
      "epoch 8, step 549, loss 313.257996\n",
      "epoch 8, step 550, loss 180.796478\n",
      "epoch 8, step 551, loss 248.071838\n",
      "epoch 8, step 552, loss 257.380768\n",
      "epoch 8, step 553, loss 187.326935\n",
      "epoch 8, step 554, loss 327.306274\n",
      "epoch 8, step 555, loss 369.673920\n",
      "epoch 8, step 556, loss 314.438782\n",
      "epoch 8, step 557, loss 198.096939\n",
      "epoch 8, step 558, loss 322.234070\n",
      "epoch 8, step 559, loss 209.566010\n",
      "epoch 8, step 560, loss 468.797974\n",
      "epoch 8, step 561, loss 182.273010\n",
      "epoch 8, step 562, loss 191.131989\n",
      "epoch 8, step 563, loss 462.264343\n",
      "epoch 8, step 564, loss 234.207687\n",
      "epoch 8, step 565, loss 151.004150\n",
      "epoch 8, step 566, loss 128.810913\n",
      "Train Weighted Classification Accuracy: 0.583146\n",
      "Valid Weighted Classification Accuracy: 0.563025\n",
      "epoch 9, step 567, loss 34.781517\n",
      "epoch 9, step 568, loss 93.606171\n",
      "epoch 9, step 569, loss 142.342758\n",
      "epoch 9, step 570, loss 91.296547\n",
      "epoch 9, step 571, loss 76.786362\n",
      "epoch 9, step 572, loss 4.430859\n",
      "epoch 9, step 573, loss 17.366882\n",
      "epoch 9, step 574, loss 25.375443\n",
      "epoch 9, step 575, loss 15.460368\n",
      "epoch 9, step 576, loss 48.778339\n",
      "epoch 9, step 577, loss 90.933517\n",
      "epoch 9, step 578, loss 230.938690\n",
      "epoch 9, step 579, loss 15.072095\n",
      "epoch 9, step 580, loss 6.813717\n",
      "epoch 9, step 581, loss 141.951675\n",
      "epoch 9, step 582, loss 183.186340\n",
      "epoch 9, step 583, loss 36.331783\n",
      "epoch 9, step 584, loss 55.754791\n",
      "epoch 9, step 585, loss 192.485031\n",
      "epoch 9, step 586, loss 31.877810\n",
      "epoch 9, step 587, loss 88.298416\n",
      "epoch 9, step 588, loss 21.345259\n",
      "epoch 9, step 589, loss 112.198341\n",
      "epoch 9, step 590, loss 141.668823\n",
      "epoch 9, step 591, loss 118.833603\n",
      "epoch 9, step 592, loss 43.551350\n",
      "epoch 9, step 593, loss 17.944450\n",
      "epoch 9, step 594, loss 52.311905\n",
      "epoch 9, step 595, loss 184.905884\n",
      "epoch 9, step 596, loss 69.385025\n",
      "epoch 9, step 597, loss 53.412598\n",
      "epoch 9, step 598, loss 53.622452\n",
      "epoch 9, step 599, loss 105.476349\n",
      "epoch 9, step 600, loss 53.237259\n",
      "epoch 9, step 601, loss 19.667948\n",
      "epoch 9, step 602, loss 107.304863\n",
      "epoch 9, step 603, loss 475.523773\n",
      "epoch 9, step 604, loss 505.724487\n",
      "epoch 9, step 605, loss 297.061096\n",
      "epoch 9, step 606, loss 119.103027\n",
      "epoch 9, step 607, loss 122.826088\n",
      "epoch 9, step 608, loss 293.251221\n",
      "epoch 9, step 609, loss 275.474426\n",
      "epoch 9, step 610, loss 131.376007\n",
      "epoch 9, step 611, loss 239.797302\n",
      "epoch 9, step 612, loss 324.836029\n",
      "epoch 9, step 613, loss 116.849701\n",
      "epoch 9, step 614, loss 412.849670\n",
      "epoch 9, step 615, loss 195.631104\n",
      "epoch 9, step 616, loss 127.742546\n",
      "epoch 9, step 617, loss 337.165466\n",
      "epoch 9, step 618, loss 334.047058\n",
      "epoch 9, step 619, loss 281.959778\n",
      "epoch 9, step 620, loss 243.098160\n",
      "epoch 9, step 621, loss 273.867859\n",
      "epoch 9, step 622, loss 120.789734\n",
      "epoch 9, step 623, loss 358.229553\n",
      "epoch 9, step 624, loss 146.907867\n",
      "epoch 9, step 625, loss 189.160583\n",
      "epoch 9, step 626, loss 314.034821\n",
      "epoch 9, step 627, loss 115.492165\n",
      "epoch 9, step 628, loss 197.587128\n",
      "epoch 9, step 629, loss 65.381981\n",
      "Train Weighted Classification Accuracy: 0.601344\n",
      "Valid Weighted Classification Accuracy: 0.563025\n",
      "Test Weighted Classification Accuracy: 0.564900\n"
     ]
    }
   ],
   "source": [
    "train_writer = tf.compat.v1.summary.FileWriter('/tmp/fcnet-tox-21',tf.compat.v1.get_default_graph())\n",
    "N = train_X.shape[0]\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    # Minibatch implementation\n",
    "    step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "            batch_X = train_X[pos:pos + batch_size]\n",
    "            batch_y = train_y[pos:pos + batch_size]\n",
    "            feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "            _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "            print(\"epoch %d, step %d, loss %f\" % (epoch, step, loss))\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "\n",
    "        train_y_pred = sess.run(y_pred, feed_dict={x: train_X, keep_prob: 1.0})\n",
    "        valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X, keep_prob: 1.0})\n",
    "        train_weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
    "        valid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "        print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "        print(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)\n",
    "\n",
    "\n",
    "    test_y_pred = sess.run(y_pred, feed_dict={x: test_X, keep_prob: 1.0})\n",
    "    test_weighted_score = accuracy_score(test_y, test_y_pred, sample_weight=test_w)\n",
    "    print(\"Test Weighted Classification Accuracy: %f\" % test_weighted_score)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "393fcab780e87c738780ceeb980b543ebdfd57cc9b80e4ffd28fcf595b13429f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('CSUG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
