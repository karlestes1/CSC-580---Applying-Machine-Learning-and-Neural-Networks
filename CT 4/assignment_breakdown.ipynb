{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC 580: Critical Thinking 4 - Toxicology Testing\n",
    "TODO - Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from deepchem import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Seeds are set via assignment parameters\n",
    "np.random.seed(456)\n",
    "tf.random.set_seed(456)\n",
    "\n",
    "# Disables eager execution so TF v1 code can be run\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data\n",
    "Using the [Tox21 Dataset](https://tox21.gov/resources/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,(train, valid, test),_ = dc.molnet.load_tox21()\n",
    "\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y[:,0]\n",
    "valid_y = valid_y[:,0]\n",
    "test_y = test_y[:,0]\n",
    "\n",
    "train_w = train_w[:,0]\n",
    "valid_w = valid_w[:,0]\n",
    "test_w = test_w[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1024 # Dimensionality of the feature vector\n",
    "n_hidden = 50\n",
    "learning_rate = .0001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "dropout_prob = 1.0\n",
    "\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.compat.v1.placeholder(tf.float32, (None, d))\n",
    "    y = tf.compat.v1.placeholder(tf.float32, (None,))\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32) # Dropout placeholder\n",
    "\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W = tf.compat.v1.Variable(tf.compat.v1.random_normal((d, n_hidden)))\n",
    "    b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden,)))\n",
    "    x_hidden = tf.compat.v1.nn.relu(tf.matmul(x,W) + b)\n",
    "    x_hidden = tf.compat.v1.nn.dropout(x_hidden, keep_prob) # Applying dropout\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden, 1)))\n",
    "    b = tf.compat.v1.Variable(tf.compat.v1.random_normal((1,)))\n",
    "    y_logit = tf.compat.v1.matmul(x_hidden,W) + b\n",
    "\n",
    "    # The sigmoid gives the class probability of 1\n",
    "    y_one_prob = tf.compat.v1.sigmoid(y_logit)\n",
    "\n",
    "    # Rounding P(y=1) will give the correct prediction\n",
    "    y_pred = tf.compat.v1.round(y_one_prob)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # Compute the cross-entropy term for each datapoint\n",
    "    y_expand = tf.expand_dims(y, 1)\n",
    "    entropy = tf.compat.v1.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "\n",
    "    # Sum all contributions\n",
    "    l = tf.compat.v1.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.compat.v1.summary.scalar(\"loss\", l)\n",
    "    merged = tf.compat.v1.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 13:26:29.443858: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 0, loss 1704.869019\n",
      "epoch 0, step 1, loss 1607.726196\n",
      "epoch 0, step 2, loss 1762.231689\n",
      "epoch 0, step 3, loss 1722.639771\n",
      "epoch 0, step 4, loss 1666.673340\n",
      "epoch 0, step 5, loss 1615.163086\n",
      "epoch 0, step 6, loss 1664.047119\n",
      "epoch 0, step 7, loss 1756.538452\n",
      "epoch 0, step 8, loss 2023.462036\n",
      "epoch 0, step 9, loss 1206.404663\n",
      "epoch 0, step 10, loss 1382.341919\n",
      "epoch 0, step 11, loss 2272.872070\n",
      "epoch 0, step 12, loss 1610.588867\n",
      "epoch 0, step 13, loss 1721.036133\n",
      "epoch 0, step 14, loss 1548.299805\n",
      "epoch 0, step 15, loss 1717.597656\n",
      "epoch 0, step 16, loss 1498.793091\n",
      "epoch 0, step 17, loss 2142.498779\n",
      "epoch 0, step 18, loss 2200.989014\n",
      "epoch 0, step 19, loss 2852.584229\n",
      "epoch 0, step 20, loss 2199.311523\n",
      "epoch 0, step 21, loss 2227.926758\n",
      "epoch 0, step 22, loss 2354.335693\n",
      "epoch 0, step 23, loss 2271.732178\n",
      "epoch 0, step 24, loss 2348.639404\n",
      "epoch 0, step 25, loss 2519.444336\n",
      "epoch 0, step 26, loss 2134.872559\n",
      "epoch 0, step 27, loss 2146.712646\n",
      "epoch 0, step 28, loss 1914.579834\n",
      "epoch 0, step 29, loss 2314.354492\n",
      "epoch 0, step 30, loss 2715.521240\n",
      "epoch 0, step 31, loss 2499.524170\n",
      "epoch 0, step 32, loss 1972.062744\n",
      "epoch 0, step 33, loss 1767.156128\n",
      "epoch 0, step 34, loss 2485.984863\n",
      "epoch 0, step 35, loss 2083.270508\n",
      "epoch 0, step 36, loss 2047.720459\n",
      "epoch 0, step 37, loss 1903.664673\n",
      "epoch 0, step 38, loss 2765.999756\n",
      "epoch 0, step 39, loss 2313.525391\n",
      "epoch 0, step 40, loss 2188.852051\n",
      "epoch 0, step 41, loss 2968.778809\n",
      "epoch 0, step 42, loss 2014.152954\n",
      "epoch 0, step 43, loss 2101.420166\n",
      "epoch 0, step 44, loss 1643.491943\n",
      "epoch 0, step 45, loss 2227.547119\n",
      "epoch 0, step 46, loss 3126.772949\n",
      "epoch 0, step 47, loss 3296.699951\n",
      "epoch 0, step 48, loss 2902.995850\n",
      "epoch 0, step 49, loss 2462.796631\n",
      "epoch 0, step 50, loss 3059.612061\n",
      "epoch 0, step 51, loss 2404.322998\n",
      "epoch 0, step 52, loss 2329.461426\n",
      "epoch 0, step 53, loss 2650.894043\n",
      "epoch 0, step 54, loss 2293.428223\n",
      "epoch 0, step 55, loss 2301.547363\n",
      "epoch 0, step 56, loss 2615.966797\n",
      "epoch 0, step 57, loss 2417.838867\n",
      "epoch 0, step 58, loss 1917.693726\n",
      "epoch 0, step 59, loss 1958.179688\n",
      "epoch 0, step 60, loss 2720.281494\n",
      "epoch 0, step 61, loss 2408.656250\n",
      "epoch 0, step 62, loss 1802.872314\n",
      "epoch 1, step 63, loss 1325.260498\n",
      "epoch 1, step 64, loss 1477.845459\n",
      "epoch 1, step 65, loss 1656.725830\n",
      "epoch 1, step 66, loss 1495.525391\n",
      "epoch 1, step 67, loss 1457.906738\n",
      "epoch 1, step 68, loss 1646.386230\n",
      "epoch 1, step 69, loss 1674.265625\n",
      "epoch 1, step 70, loss 1964.519531\n",
      "epoch 1, step 71, loss 1788.635742\n",
      "epoch 1, step 72, loss 1413.964600\n",
      "epoch 1, step 73, loss 1880.827881\n",
      "epoch 1, step 74, loss 1805.319946\n",
      "epoch 1, step 75, loss 1578.036621\n",
      "epoch 1, step 76, loss 1329.739502\n",
      "epoch 1, step 77, loss 1480.012573\n",
      "epoch 1, step 78, loss 1531.364502\n",
      "epoch 1, step 79, loss 1629.056885\n",
      "epoch 1, step 80, loss 1499.107544\n",
      "epoch 1, step 81, loss 2445.822754\n",
      "epoch 1, step 82, loss 1891.046997\n",
      "epoch 1, step 83, loss 2086.880859\n",
      "epoch 1, step 84, loss 2110.121094\n",
      "epoch 1, step 85, loss 2350.596680\n",
      "epoch 1, step 86, loss 2346.313721\n",
      "epoch 1, step 87, loss 2141.944336\n",
      "epoch 1, step 88, loss 2383.638184\n",
      "epoch 1, step 89, loss 2057.369629\n",
      "epoch 1, step 90, loss 2240.365234\n",
      "epoch 1, step 91, loss 1808.190918\n",
      "epoch 1, step 92, loss 1974.069336\n",
      "epoch 1, step 93, loss 1854.702515\n",
      "epoch 1, step 94, loss 2028.707520\n",
      "epoch 1, step 95, loss 1945.783203\n",
      "epoch 1, step 96, loss 2012.091797\n",
      "epoch 1, step 97, loss 2141.103271\n",
      "epoch 1, step 98, loss 1822.029663\n",
      "epoch 1, step 99, loss 1403.624512\n",
      "epoch 1, step 100, loss 2166.017090\n",
      "epoch 1, step 101, loss 3391.485596\n",
      "epoch 1, step 102, loss 2269.196777\n",
      "epoch 1, step 103, loss 2258.394531\n",
      "epoch 1, step 104, loss 2177.594238\n",
      "epoch 1, step 105, loss 2009.581787\n",
      "epoch 1, step 106, loss 2052.572021\n",
      "epoch 1, step 107, loss 1682.699097\n",
      "epoch 1, step 108, loss 2448.244629\n",
      "epoch 1, step 109, loss 2605.463867\n",
      "epoch 1, step 110, loss 2429.450195\n",
      "epoch 1, step 111, loss 2359.059082\n",
      "epoch 1, step 112, loss 2507.203369\n",
      "epoch 1, step 113, loss 2526.462646\n",
      "epoch 1, step 114, loss 2556.687500\n",
      "epoch 1, step 115, loss 2219.216309\n",
      "epoch 1, step 116, loss 2294.141846\n",
      "epoch 1, step 117, loss 2543.159912\n",
      "epoch 1, step 118, loss 2371.548828\n",
      "epoch 1, step 119, loss 1811.954956\n",
      "epoch 1, step 120, loss 2041.262939\n",
      "epoch 1, step 121, loss 2128.471680\n",
      "epoch 1, step 122, loss 1831.110840\n",
      "epoch 1, step 123, loss 2638.426514\n",
      "epoch 1, step 124, loss 2189.417969\n",
      "epoch 1, step 125, loss 1043.647705\n",
      "epoch 2, step 126, loss 1347.291382\n",
      "epoch 2, step 127, loss 1598.495361\n",
      "epoch 2, step 128, loss 1768.964600\n",
      "epoch 2, step 129, loss 1529.417725\n",
      "epoch 2, step 130, loss 1967.837524\n",
      "epoch 2, step 131, loss 1279.400879\n",
      "epoch 2, step 132, loss 1749.049316\n",
      "epoch 2, step 133, loss 1515.869629\n",
      "epoch 2, step 134, loss 1796.635254\n",
      "epoch 2, step 135, loss 1482.020386\n",
      "epoch 2, step 136, loss 1449.054565\n",
      "epoch 2, step 137, loss 1855.702515\n",
      "epoch 2, step 138, loss 1891.032104\n",
      "epoch 2, step 139, loss 1454.201172\n",
      "epoch 2, step 140, loss 1515.820801\n",
      "epoch 2, step 141, loss 1410.370728\n",
      "epoch 2, step 142, loss 1309.035278\n",
      "epoch 2, step 143, loss 1785.041260\n",
      "epoch 2, step 144, loss 2029.941895\n",
      "epoch 2, step 145, loss 1871.149780\n",
      "epoch 2, step 146, loss 1965.317017\n",
      "epoch 2, step 147, loss 2118.450195\n",
      "epoch 2, step 148, loss 1843.554321\n",
      "epoch 2, step 149, loss 2104.709717\n",
      "epoch 2, step 150, loss 1821.217407\n",
      "epoch 2, step 151, loss 2019.096924\n",
      "epoch 2, step 152, loss 1888.797852\n",
      "epoch 2, step 153, loss 1757.926025\n",
      "epoch 2, step 154, loss 1695.670654\n",
      "epoch 2, step 155, loss 2009.475952\n",
      "epoch 2, step 156, loss 1913.516113\n",
      "epoch 2, step 157, loss 2106.740234\n",
      "epoch 2, step 158, loss 1883.019287\n",
      "epoch 2, step 159, loss 2676.701172\n",
      "epoch 2, step 160, loss 2050.849609\n",
      "epoch 2, step 161, loss 1599.357178\n",
      "epoch 2, step 162, loss 1655.550293\n",
      "epoch 2, step 163, loss 1864.768433\n",
      "epoch 2, step 164, loss 1900.466309\n",
      "epoch 2, step 165, loss 2654.016113\n",
      "epoch 2, step 166, loss 1978.472900\n",
      "epoch 2, step 167, loss 2496.486816\n",
      "epoch 2, step 168, loss 2042.745605\n",
      "epoch 2, step 169, loss 2111.458008\n",
      "epoch 2, step 170, loss 2074.802734\n",
      "epoch 2, step 171, loss 1886.693848\n",
      "epoch 2, step 172, loss 2505.969238\n",
      "epoch 2, step 173, loss 2513.483643\n",
      "epoch 2, step 174, loss 2264.919678\n",
      "epoch 2, step 175, loss 2413.670654\n",
      "epoch 2, step 176, loss 2444.011230\n",
      "epoch 2, step 177, loss 2215.474609\n",
      "epoch 2, step 178, loss 1856.022705\n",
      "epoch 2, step 179, loss 2553.677979\n",
      "epoch 2, step 180, loss 2253.108398\n",
      "epoch 2, step 181, loss 2267.532715\n",
      "epoch 2, step 182, loss 2533.371582\n",
      "epoch 2, step 183, loss 2516.242920\n",
      "epoch 2, step 184, loss 1569.524170\n",
      "epoch 2, step 185, loss 2475.480469\n",
      "epoch 2, step 186, loss 2449.084229\n",
      "epoch 2, step 187, loss 1625.566162\n",
      "epoch 2, step 188, loss 1418.680908\n",
      "epoch 3, step 189, loss 1328.746338\n",
      "epoch 3, step 190, loss 1469.903320\n",
      "epoch 3, step 191, loss 1699.387695\n",
      "epoch 3, step 192, loss 1204.351318\n",
      "epoch 3, step 193, loss 1473.425781\n",
      "epoch 3, step 194, loss 1556.296631\n",
      "epoch 3, step 195, loss 1819.583252\n",
      "epoch 3, step 196, loss 1466.384766\n",
      "epoch 3, step 197, loss 1252.597168\n",
      "epoch 3, step 198, loss 1625.911133\n",
      "epoch 3, step 199, loss 1128.479004\n",
      "epoch 3, step 200, loss 1686.755737\n",
      "epoch 3, step 201, loss 1264.149170\n",
      "epoch 3, step 202, loss 1358.710449\n",
      "epoch 3, step 203, loss 1361.597046\n",
      "epoch 3, step 204, loss 2189.061768\n",
      "epoch 3, step 205, loss 1699.728882\n",
      "epoch 3, step 206, loss 1886.428589\n",
      "epoch 3, step 207, loss 2107.788330\n",
      "epoch 3, step 208, loss 2378.888184\n",
      "epoch 3, step 209, loss 2375.350830\n",
      "epoch 3, step 210, loss 1875.969727\n",
      "epoch 3, step 211, loss 2021.243652\n",
      "epoch 3, step 212, loss 1922.463989\n",
      "epoch 3, step 213, loss 2033.091553\n",
      "epoch 3, step 214, loss 2150.514160\n",
      "epoch 3, step 215, loss 2319.176025\n",
      "epoch 3, step 216, loss 1644.889526\n",
      "epoch 3, step 217, loss 1740.551147\n",
      "epoch 3, step 218, loss 1952.438477\n",
      "epoch 3, step 219, loss 1964.501343\n",
      "epoch 3, step 220, loss 1815.953125\n",
      "epoch 3, step 221, loss 1885.483521\n",
      "epoch 3, step 222, loss 2137.126465\n",
      "epoch 3, step 223, loss 1820.133667\n",
      "epoch 3, step 224, loss 1295.133667\n",
      "epoch 3, step 225, loss 1528.289307\n",
      "epoch 3, step 226, loss 2209.626221\n",
      "epoch 3, step 227, loss 2254.894287\n",
      "epoch 3, step 228, loss 1977.248047\n",
      "epoch 3, step 229, loss 1690.268188\n",
      "epoch 3, step 230, loss 2731.654053\n",
      "epoch 3, step 231, loss 2026.876465\n",
      "epoch 3, step 232, loss 2059.796875\n",
      "epoch 3, step 233, loss 1939.486328\n",
      "epoch 3, step 234, loss 1693.048706\n",
      "epoch 3, step 235, loss 1945.808472\n",
      "epoch 3, step 236, loss 2174.399414\n",
      "epoch 3, step 237, loss 2170.708008\n",
      "epoch 3, step 238, loss 2209.795898\n",
      "epoch 3, step 239, loss 2276.515137\n",
      "epoch 3, step 240, loss 1824.092529\n",
      "epoch 3, step 241, loss 1913.596436\n",
      "epoch 3, step 242, loss 2566.431152\n",
      "epoch 3, step 243, loss 2375.534912\n",
      "epoch 3, step 244, loss 2297.597168\n",
      "epoch 3, step 245, loss 2313.079102\n",
      "epoch 3, step 246, loss 2279.437988\n",
      "epoch 3, step 247, loss 2000.613892\n",
      "epoch 3, step 248, loss 1720.778076\n",
      "epoch 3, step 249, loss 1947.390747\n",
      "epoch 3, step 250, loss 2494.480957\n",
      "epoch 3, step 251, loss 1169.776855\n",
      "epoch 4, step 252, loss 1401.437500\n",
      "epoch 4, step 253, loss 1618.153931\n",
      "epoch 4, step 254, loss 1022.302307\n",
      "epoch 4, step 255, loss 1567.548706\n",
      "epoch 4, step 256, loss 1473.361816\n",
      "epoch 4, step 257, loss 1720.264404\n",
      "epoch 4, step 258, loss 1043.442505\n",
      "epoch 4, step 259, loss 1519.723267\n",
      "epoch 4, step 260, loss 1606.567505\n",
      "epoch 4, step 261, loss 1129.011230\n",
      "epoch 4, step 262, loss 1411.331543\n",
      "epoch 4, step 263, loss 1712.730347\n",
      "epoch 4, step 264, loss 1344.023804\n",
      "epoch 4, step 265, loss 1650.854980\n",
      "epoch 4, step 266, loss 1460.109131\n",
      "epoch 4, step 267, loss 1651.539307\n",
      "epoch 4, step 268, loss 1034.737793\n",
      "epoch 4, step 269, loss 1393.810791\n",
      "epoch 4, step 270, loss 2096.689209\n",
      "epoch 4, step 271, loss 1843.605469\n",
      "epoch 4, step 272, loss 1590.152344\n",
      "epoch 4, step 273, loss 1683.638184\n",
      "epoch 4, step 274, loss 1917.302979\n",
      "epoch 4, step 275, loss 2002.415649\n",
      "epoch 4, step 276, loss 1562.645996\n",
      "epoch 4, step 277, loss 2241.949219\n",
      "epoch 4, step 278, loss 1929.314209\n",
      "epoch 4, step 279, loss 2012.299561\n",
      "epoch 4, step 280, loss 2397.008057\n",
      "epoch 4, step 281, loss 1963.492065\n",
      "epoch 4, step 282, loss 1847.136597\n",
      "epoch 4, step 283, loss 2084.816406\n",
      "epoch 4, step 284, loss 1903.266113\n",
      "epoch 4, step 285, loss 2129.934082\n",
      "epoch 4, step 286, loss 1754.443848\n",
      "epoch 4, step 287, loss 1279.382080\n",
      "epoch 4, step 288, loss 1848.763184\n",
      "epoch 4, step 289, loss 2232.545166\n",
      "epoch 4, step 290, loss 1846.697754\n",
      "epoch 4, step 291, loss 1912.541992\n",
      "epoch 4, step 292, loss 1991.495117\n",
      "epoch 4, step 293, loss 2021.160156\n",
      "epoch 4, step 294, loss 1706.977295\n",
      "epoch 4, step 295, loss 2164.423584\n",
      "epoch 4, step 296, loss 1885.793701\n",
      "epoch 4, step 297, loss 1586.171753\n",
      "epoch 4, step 298, loss 1592.731689\n",
      "epoch 4, step 299, loss 2290.902100\n",
      "epoch 4, step 300, loss 2270.929688\n",
      "epoch 4, step 301, loss 2534.239746\n",
      "epoch 4, step 302, loss 2035.734375\n",
      "epoch 4, step 303, loss 1976.059326\n",
      "epoch 4, step 304, loss 2277.839355\n",
      "epoch 4, step 305, loss 2689.080566\n",
      "epoch 4, step 306, loss 2262.331787\n",
      "epoch 4, step 307, loss 2053.652832\n",
      "epoch 4, step 308, loss 2033.148926\n",
      "epoch 4, step 309, loss 2161.523438\n",
      "epoch 4, step 310, loss 2054.107178\n",
      "epoch 4, step 311, loss 1939.336548\n",
      "epoch 4, step 312, loss 2067.908203\n",
      "epoch 4, step 313, loss 1720.078857\n",
      "epoch 4, step 314, loss 1636.607422\n",
      "epoch 5, step 315, loss 1301.407104\n",
      "epoch 5, step 316, loss 1501.172119\n",
      "epoch 5, step 317, loss 1234.570190\n",
      "epoch 5, step 318, loss 1568.819336\n",
      "epoch 5, step 319, loss 1323.872070\n",
      "epoch 5, step 320, loss 1657.975342\n",
      "epoch 5, step 321, loss 1420.921997\n",
      "epoch 5, step 322, loss 1275.204834\n",
      "epoch 5, step 323, loss 1718.590576\n",
      "epoch 5, step 324, loss 1018.654419\n",
      "epoch 5, step 325, loss 1306.559814\n",
      "epoch 5, step 326, loss 1295.908081\n",
      "epoch 5, step 327, loss 1006.168823\n",
      "epoch 5, step 328, loss 1317.288818\n",
      "epoch 5, step 329, loss 1323.116333\n",
      "epoch 5, step 330, loss 1192.213379\n",
      "epoch 5, step 331, loss 1522.844971\n",
      "epoch 5, step 332, loss 1933.344727\n",
      "epoch 5, step 333, loss 1778.887573\n",
      "epoch 5, step 334, loss 2330.824219\n",
      "epoch 5, step 335, loss 1734.877930\n",
      "epoch 5, step 336, loss 1627.799072\n",
      "epoch 5, step 337, loss 1458.541504\n",
      "epoch 5, step 338, loss 2040.877197\n",
      "epoch 5, step 339, loss 2074.587402\n",
      "epoch 5, step 340, loss 2088.956543\n",
      "epoch 5, step 341, loss 1298.448242\n",
      "epoch 5, step 342, loss 1562.906006\n",
      "epoch 5, step 343, loss 1778.906738\n",
      "epoch 5, step 344, loss 1779.559692\n",
      "epoch 5, step 345, loss 1559.980957\n",
      "epoch 5, step 346, loss 1909.945068\n",
      "epoch 5, step 347, loss 1634.458130\n",
      "epoch 5, step 348, loss 1945.789307\n",
      "epoch 5, step 349, loss 1841.380737\n",
      "epoch 5, step 350, loss 1152.027588\n",
      "epoch 5, step 351, loss 1834.295044\n",
      "epoch 5, step 352, loss 2535.145752\n",
      "epoch 5, step 353, loss 1982.503418\n",
      "epoch 5, step 354, loss 1703.517578\n",
      "epoch 5, step 355, loss 1472.594971\n",
      "epoch 5, step 356, loss 1874.619141\n",
      "epoch 5, step 357, loss 1655.568604\n",
      "epoch 5, step 358, loss 2447.033447\n",
      "epoch 5, step 359, loss 1762.741089\n",
      "epoch 5, step 360, loss 1674.683105\n",
      "epoch 5, step 361, loss 1678.418335\n",
      "epoch 5, step 362, loss 2000.076782\n",
      "epoch 5, step 363, loss 2142.648926\n",
      "epoch 5, step 364, loss 2206.267090\n",
      "epoch 5, step 365, loss 1933.418945\n",
      "epoch 5, step 366, loss 1395.978149\n",
      "epoch 5, step 367, loss 1653.840576\n",
      "epoch 5, step 368, loss 2145.925293\n",
      "epoch 5, step 369, loss 1499.378906\n",
      "epoch 5, step 370, loss 2160.176758\n",
      "epoch 5, step 371, loss 2003.781738\n",
      "epoch 5, step 372, loss 1720.213745\n",
      "epoch 5, step 373, loss 1931.035034\n",
      "epoch 5, step 374, loss 1835.553589\n",
      "epoch 5, step 375, loss 1553.852783\n",
      "epoch 5, step 376, loss 1993.333862\n",
      "epoch 5, step 377, loss 981.489624\n",
      "epoch 6, step 378, loss 1480.778564\n",
      "epoch 6, step 379, loss 1574.748047\n",
      "epoch 6, step 380, loss 1172.410278\n",
      "epoch 6, step 381, loss 1488.781372\n",
      "epoch 6, step 382, loss 1746.157959\n",
      "epoch 6, step 383, loss 934.081543\n",
      "epoch 6, step 384, loss 1310.456787\n",
      "epoch 6, step 385, loss 991.774170\n",
      "epoch 6, step 386, loss 1975.447021\n",
      "epoch 6, step 387, loss 958.141052\n",
      "epoch 6, step 388, loss 1625.569580\n",
      "epoch 6, step 389, loss 1420.245117\n",
      "epoch 6, step 390, loss 1510.717041\n",
      "epoch 6, step 391, loss 1070.408081\n",
      "epoch 6, step 392, loss 1113.045166\n",
      "epoch 6, step 393, loss 810.270508\n",
      "epoch 6, step 394, loss 1252.675293\n",
      "epoch 6, step 395, loss 1328.511597\n",
      "epoch 6, step 396, loss 1478.091309\n",
      "epoch 6, step 397, loss 1404.865601\n",
      "epoch 6, step 398, loss 1815.623657\n",
      "epoch 6, step 399, loss 1447.171021\n",
      "epoch 6, step 400, loss 1503.657227\n",
      "epoch 6, step 401, loss 1650.279785\n",
      "epoch 6, step 402, loss 1362.078979\n",
      "epoch 6, step 403, loss 2586.369873\n",
      "epoch 6, step 404, loss 1381.090942\n",
      "epoch 6, step 405, loss 1803.812012\n",
      "epoch 6, step 406, loss 2111.231201\n",
      "epoch 6, step 407, loss 1586.375122\n",
      "epoch 6, step 408, loss 1265.652832\n",
      "epoch 6, step 409, loss 1180.355469\n",
      "epoch 6, step 410, loss 1424.557373\n",
      "epoch 6, step 411, loss 1847.288940\n",
      "epoch 6, step 412, loss 1893.290039\n",
      "epoch 6, step 413, loss 1407.300415\n",
      "epoch 6, step 414, loss 1347.931641\n",
      "epoch 6, step 415, loss 1363.237793\n",
      "epoch 6, step 416, loss 1917.831909\n",
      "epoch 6, step 417, loss 1915.171387\n",
      "epoch 6, step 418, loss 1730.545898\n",
      "epoch 6, step 419, loss 2128.806885\n",
      "epoch 6, step 420, loss 1736.428955\n",
      "epoch 6, step 421, loss 1348.598267\n",
      "epoch 6, step 422, loss 1634.518066\n",
      "epoch 6, step 423, loss 1522.765259\n",
      "epoch 6, step 424, loss 1559.471436\n",
      "epoch 6, step 425, loss 2318.385742\n",
      "epoch 6, step 426, loss 2070.403809\n",
      "epoch 6, step 427, loss 2757.782227\n",
      "epoch 6, step 428, loss 2228.498291\n",
      "epoch 6, step 429, loss 1683.339844\n",
      "epoch 6, step 430, loss 1930.539673\n",
      "epoch 6, step 431, loss 1643.990723\n",
      "epoch 6, step 432, loss 1477.650879\n",
      "epoch 6, step 433, loss 1580.514893\n",
      "epoch 6, step 434, loss 1233.133301\n",
      "epoch 6, step 435, loss 2133.542969\n",
      "epoch 6, step 436, loss 1553.107178\n",
      "epoch 6, step 437, loss 1573.524414\n",
      "epoch 6, step 438, loss 1934.057861\n",
      "epoch 6, step 439, loss 1758.956177\n",
      "epoch 6, step 440, loss 858.458984\n",
      "epoch 7, step 441, loss 1219.039062\n",
      "epoch 7, step 442, loss 1187.545654\n",
      "epoch 7, step 443, loss 1249.226807\n",
      "epoch 7, step 444, loss 1161.363525\n",
      "epoch 7, step 445, loss 1319.309692\n",
      "epoch 7, step 446, loss 926.171265\n",
      "epoch 7, step 447, loss 1046.427246\n",
      "epoch 7, step 448, loss 1202.637573\n",
      "epoch 7, step 449, loss 886.869019\n",
      "epoch 7, step 450, loss 978.300415\n",
      "epoch 7, step 451, loss 1045.061768\n",
      "epoch 7, step 452, loss 1237.124634\n",
      "epoch 7, step 453, loss 1061.013062\n",
      "epoch 7, step 454, loss 1330.401001\n",
      "epoch 7, step 455, loss 1279.551270\n",
      "epoch 7, step 456, loss 1228.213623\n",
      "epoch 7, step 457, loss 1457.843262\n",
      "epoch 7, step 458, loss 1539.657471\n",
      "epoch 7, step 459, loss 1551.989502\n",
      "epoch 7, step 460, loss 2234.434570\n",
      "epoch 7, step 461, loss 1391.962402\n",
      "epoch 7, step 462, loss 1438.827393\n",
      "epoch 7, step 463, loss 1793.003906\n",
      "epoch 7, step 464, loss 1741.471436\n",
      "epoch 7, step 465, loss 1388.475342\n",
      "epoch 7, step 466, loss 1647.391846\n",
      "epoch 7, step 467, loss 1265.565918\n",
      "epoch 7, step 468, loss 1701.690918\n",
      "epoch 7, step 469, loss 1801.032959\n",
      "epoch 7, step 470, loss 1165.980957\n",
      "epoch 7, step 471, loss 1573.569824\n",
      "epoch 7, step 472, loss 2076.389404\n",
      "epoch 7, step 473, loss 1342.454956\n",
      "epoch 7, step 474, loss 1389.088623\n",
      "epoch 7, step 475, loss 1585.973022\n",
      "epoch 7, step 476, loss 1326.580933\n",
      "epoch 7, step 477, loss 1595.871704\n",
      "epoch 7, step 478, loss 1544.378784\n",
      "epoch 7, step 479, loss 1722.912109\n",
      "epoch 7, step 480, loss 1709.876465\n",
      "epoch 7, step 481, loss 1385.055420\n",
      "epoch 7, step 482, loss 1549.500244\n",
      "epoch 7, step 483, loss 1980.731079\n",
      "epoch 7, step 484, loss 1427.133301\n",
      "epoch 7, step 485, loss 1423.116699\n",
      "epoch 7, step 486, loss 1538.010864\n",
      "epoch 7, step 487, loss 1535.288086\n",
      "epoch 7, step 488, loss 1844.339844\n",
      "epoch 7, step 489, loss 1561.355469\n",
      "epoch 7, step 490, loss 2326.191162\n",
      "epoch 7, step 491, loss 1491.990112\n",
      "epoch 7, step 492, loss 1797.699463\n",
      "epoch 7, step 493, loss 1579.497070\n",
      "epoch 7, step 494, loss 1654.070557\n",
      "epoch 7, step 495, loss 1523.717773\n",
      "epoch 7, step 496, loss 2162.598633\n",
      "epoch 7, step 497, loss 1844.407471\n",
      "epoch 7, step 498, loss 2120.877930\n",
      "epoch 7, step 499, loss 1560.140381\n",
      "epoch 7, step 500, loss 1773.204712\n",
      "epoch 7, step 501, loss 1411.436768\n",
      "epoch 7, step 502, loss 1998.440430\n",
      "epoch 7, step 503, loss 1552.401611\n",
      "epoch 8, step 504, loss 989.484802\n",
      "epoch 8, step 505, loss 1144.845215\n",
      "epoch 8, step 506, loss 913.998840\n",
      "epoch 8, step 507, loss 1217.363037\n",
      "epoch 8, step 508, loss 1083.342285\n",
      "epoch 8, step 509, loss 1284.239136\n",
      "epoch 8, step 510, loss 1239.978882\n",
      "epoch 8, step 511, loss 1008.681702\n",
      "epoch 8, step 512, loss 1230.439941\n",
      "epoch 8, step 513, loss 1096.456665\n",
      "epoch 8, step 514, loss 1063.580688\n",
      "epoch 8, step 515, loss 1410.650635\n",
      "epoch 8, step 516, loss 939.937683\n",
      "epoch 8, step 517, loss 1232.578979\n",
      "epoch 8, step 518, loss 1194.128906\n",
      "epoch 8, step 519, loss 1302.281372\n",
      "epoch 8, step 520, loss 1341.817139\n",
      "epoch 8, step 521, loss 1455.157715\n",
      "epoch 8, step 522, loss 1269.745117\n",
      "epoch 8, step 523, loss 1959.501465\n",
      "epoch 8, step 524, loss 1579.961060\n",
      "epoch 8, step 525, loss 1303.499512\n",
      "epoch 8, step 526, loss 1362.993164\n",
      "epoch 8, step 527, loss 2081.228027\n",
      "epoch 8, step 528, loss 1589.600830\n",
      "epoch 8, step 529, loss 1334.317261\n",
      "epoch 8, step 530, loss 1188.645264\n",
      "epoch 8, step 531, loss 1440.558960\n",
      "epoch 8, step 532, loss 2066.728516\n",
      "epoch 8, step 533, loss 1525.098145\n",
      "epoch 8, step 534, loss 1942.183960\n",
      "epoch 8, step 535, loss 1454.652710\n",
      "epoch 8, step 536, loss 1386.325928\n",
      "epoch 8, step 537, loss 1467.083374\n",
      "epoch 8, step 538, loss 1267.726807\n",
      "epoch 8, step 539, loss 1088.188965\n",
      "epoch 8, step 540, loss 1487.009277\n",
      "epoch 8, step 541, loss 2239.591309\n",
      "epoch 8, step 542, loss 1638.083862\n",
      "epoch 8, step 543, loss 1705.212646\n",
      "epoch 8, step 544, loss 1342.627686\n",
      "epoch 8, step 545, loss 2197.573975\n",
      "epoch 8, step 546, loss 1763.185059\n",
      "epoch 8, step 547, loss 1811.608154\n",
      "epoch 8, step 548, loss 1113.702271\n",
      "epoch 8, step 549, loss 1375.242920\n",
      "epoch 8, step 550, loss 1571.254150\n",
      "epoch 8, step 551, loss 1909.953247\n",
      "epoch 8, step 552, loss 1843.473145\n",
      "epoch 8, step 553, loss 2387.166016\n",
      "epoch 8, step 554, loss 1640.593506\n",
      "epoch 8, step 555, loss 1800.649048\n",
      "epoch 8, step 556, loss 1729.878662\n",
      "epoch 8, step 557, loss 1802.300171\n",
      "epoch 8, step 558, loss 1317.002930\n",
      "epoch 8, step 559, loss 1469.476562\n",
      "epoch 8, step 560, loss 1386.011353\n",
      "epoch 8, step 561, loss 1665.159668\n",
      "epoch 8, step 562, loss 1475.807739\n",
      "epoch 8, step 563, loss 1779.509033\n",
      "epoch 8, step 564, loss 1912.790283\n",
      "epoch 8, step 565, loss 1953.555908\n",
      "epoch 8, step 566, loss 1012.193054\n",
      "epoch 9, step 567, loss 925.841309\n",
      "epoch 9, step 568, loss 1293.718262\n",
      "epoch 9, step 569, loss 1556.721436\n",
      "epoch 9, step 570, loss 1387.225830\n",
      "epoch 9, step 571, loss 1170.699951\n",
      "epoch 9, step 572, loss 1079.830200\n",
      "epoch 9, step 573, loss 1277.092896\n",
      "epoch 9, step 574, loss 1027.863037\n",
      "epoch 9, step 575, loss 747.491699\n",
      "epoch 9, step 576, loss 960.247925\n",
      "epoch 9, step 577, loss 1134.204590\n",
      "epoch 9, step 578, loss 1057.032104\n",
      "epoch 9, step 579, loss 1461.110718\n",
      "epoch 9, step 580, loss 1378.919678\n",
      "epoch 9, step 581, loss 1284.263916\n",
      "epoch 9, step 582, loss 1015.586670\n",
      "epoch 9, step 583, loss 1391.678467\n",
      "epoch 9, step 584, loss 1122.347290\n",
      "epoch 9, step 585, loss 1531.089355\n",
      "epoch 9, step 586, loss 1693.082764\n",
      "epoch 9, step 587, loss 1727.516479\n",
      "epoch 9, step 588, loss 1277.826294\n",
      "epoch 9, step 589, loss 1294.799927\n",
      "epoch 9, step 590, loss 1163.176392\n",
      "epoch 9, step 591, loss 1219.122314\n",
      "epoch 9, step 592, loss 1113.219727\n",
      "epoch 9, step 593, loss 1057.363647\n",
      "epoch 9, step 594, loss 1358.782959\n",
      "epoch 9, step 595, loss 1628.430176\n",
      "epoch 9, step 596, loss 1295.199463\n",
      "epoch 9, step 597, loss 971.062988\n",
      "epoch 9, step 598, loss 1309.154785\n",
      "epoch 9, step 599, loss 1255.477051\n",
      "epoch 9, step 600, loss 1394.476074\n",
      "epoch 9, step 601, loss 1167.710938\n",
      "epoch 9, step 602, loss 918.996582\n",
      "epoch 9, step 603, loss 1705.353638\n",
      "epoch 9, step 604, loss 2702.689941\n",
      "epoch 9, step 605, loss 1670.175415\n",
      "epoch 9, step 606, loss 1870.515625\n",
      "epoch 9, step 607, loss 1361.347290\n",
      "epoch 9, step 608, loss 1317.980347\n",
      "epoch 9, step 609, loss 1270.634399\n",
      "epoch 9, step 610, loss 1656.264404\n",
      "epoch 9, step 611, loss 1561.273071\n",
      "epoch 9, step 612, loss 1334.030029\n",
      "epoch 9, step 613, loss 1334.792480\n",
      "epoch 9, step 614, loss 2215.376953\n",
      "epoch 9, step 615, loss 1417.079590\n",
      "epoch 9, step 616, loss 1786.797852\n",
      "epoch 9, step 617, loss 1528.329224\n",
      "epoch 9, step 618, loss 1880.048218\n",
      "epoch 9, step 619, loss 1302.792725\n",
      "epoch 9, step 620, loss 1374.839478\n",
      "epoch 9, step 621, loss 1085.141113\n",
      "epoch 9, step 622, loss 1570.938354\n",
      "epoch 9, step 623, loss 1615.819824\n",
      "epoch 9, step 624, loss 1377.057861\n",
      "epoch 9, step 625, loss 1429.833130\n",
      "epoch 9, step 626, loss 1247.633545\n",
      "epoch 9, step 627, loss 1296.004150\n",
      "epoch 9, step 628, loss 1462.886475\n",
      "epoch 9, step 629, loss 1274.335327\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6264, 783]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qs/xnqw4r0x1gq1j6s7k6vmnk4h0000gn/T/ipykernel_16062/770781868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_weighted_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# valid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Weighted Classification Accuracy: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_weighted_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/CSUG/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/CSUG/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/CSUG/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6264, 783]"
     ]
    }
   ],
   "source": [
    "train_writer = tf.compat.v1.summary.FileWriter('/tmp/fcnet-tox-21',tf.compat.v1.get_default_graph())\n",
    "N = train_X.shape[0]\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    # Minibatch implementation\n",
    "    step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "            batch_X = train_X[pos:pos + batch_size]\n",
    "            batch_y = train_y[pos:pos + batch_size]\n",
    "            feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "            _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "            print(\"epoch %d, step %d, loss %f\" % (epoch, step, loss))\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "\n",
    "    train_y_pred = sess.run(y_pred, feed_dict={x: valid_X, keep_prob: 1.0})\n",
    "    # valid_y_pred = sess.run(valid_y, feed_dict={x: valid_X})\n",
    "\n",
    "    train_weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
    "    # valid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "    print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "    # print(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "393fcab780e87c738780ceeb980b543ebdfd57cc9b80e4ffd28fcf595b13429f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('CSUG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
